{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gradient-based-gh.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**GRADIENT-BASED PERTURBATIONS**\n",
        "</br> Code to perturb input data based on gradients of the loss"
      ],
      "metadata": {
        "id": "tKrJUWaKldUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TOP_K=0.5\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
        "def get_gradients(model, seq, attn_masks, token_type_ids, labels):\n",
        "    \"\"\"\n",
        "    Get gradients by applying backward hook\n",
        "    \"\"\"\n",
        "    extracted_grads = {}\n",
        "    model.eval()\n",
        "    \n",
        "    def extract_grad_hook(module, grad_in, grad_out):\n",
        "        \n",
        "        extracted_grads['embed'] = grad_out[0]\n",
        "\n",
        "    def add_hooks(bert_model):\n",
        "        module = bert_model.embeddings.word_embeddings\n",
        "        module.requires_grad_()\n",
        "        module.register_backward_hook(extract_grad_hook)\n",
        "        \n",
        "    try:\n",
        "      add_hooks(model.bert_layer)\n",
        "      \n",
        "    except NotImplemented:\n",
        "      raise NotImplementedError\n",
        "      \n",
        "    prob = model(seq, attn_masks, token_type_ids)\n",
        "    loss = criterion(prob, labels)\n",
        "    \n",
        "    loss.backward()\n",
        "    return extracted_grads['embed']\n",
        "\n",
        "def get_importance_order(model, grads, seq, attn_masks, token_type_ids, labels, is_pair=True):\n",
        "\n",
        "    # Returns indices sorted by their importance (most - > least)\n",
        "    embeds = model.bert_layer.embeddings.word_embeddings(seq)\n",
        "    one_hot_grad = -torch.mul(embeds, grads)\n",
        "             \n",
        "\n",
        "\n",
        "    importance_order_bert1 = []\n",
        "    importance_order_bert2 = []\n",
        "    bert1_tok = []\n",
        "    bert2_tok = []\n",
        "    bert1_grads = []\n",
        "    bert2_grads = []\n",
        "\n",
        "    \n",
        "    for i in range(seq.shape[0]):\n",
        "        length2 = (token_type_ids[i]).sum() - 1           # length of second sentence\n",
        "        length1 = (attn_masks[i]).sum() - 3 - length2     # length of first sentence\n",
        "        \n",
        "        bert1 = seq[i][1:1 + length1]\n",
        "        bert2 = seq[i][length1 + 2 : length1 + 2 + length2]\n",
        "        grads1 = one_hot_grad[i][1:1 + length1]\n",
        "        grads2 = one_hot_grad[i][length1 + 2 : length1 + 2 + length2]\n",
        "        bert1_tok.append(bert1)\n",
        "        bert2_tok.append(bert2)\n",
        "        bert1_grads.append(grads1)\n",
        "        bert2_grads.append(grads2)\n",
        "        order = np.argsort(grads1.sum(-1).data.cpu().numpy())\n",
        "        importance_order_bert1.append(list(order))\n",
        "        order = np.argsort(grads2.sum(-1).data.cpu().numpy())\n",
        "        importance_order_bert2.append(list(order))\n",
        "\n",
        "    return importance_order_bert1, importance_order_bert2, bert1_tok, bert2_tok, bert1_grads, bert2_grads\n",
        "\n",
        "\n",
        "def keep_most_important_words(importance_order, bert_ids, special_ids, **kwargs):\n",
        "\n",
        "    if 'topk' not in kwargs:\n",
        "        kwargs['topk'] = 0.5\n",
        "\n",
        "    # importance_order gives index, not the actual token_ids\n",
        "\n",
        "    new_bert = []\n",
        "    importance_order_ids = []\n",
        "\n",
        "    for idx in importance_order:\n",
        "        if bert_ids.tolist()[idx] not in special_ids:\n",
        "            importance_order_ids.append(bert_ids.tolist()[idx])\n",
        "\n",
        "    to_keep = math.ceil(len(importance_order_ids) * kwargs['topk'])\n",
        "    to_keep_ids = importance_order_ids[:to_keep]\n",
        "\n",
        "\n",
        "    toks = bert_ids.tolist()\n",
        "    new_toks = []\n",
        "\n",
        "    keep_ids = []\n",
        "    keep_ids.extend(special_ids)\n",
        "    keep_ids.extend(to_keep_ids)\n",
        "    for t in toks:\n",
        "        if t not in keep_ids:\n",
        "            continue\n",
        "        else:\n",
        "            new_toks.append(t)\n",
        "    new_bert.append(new_toks)\n",
        "\n",
        "    # Wont return tensor, because we can have different length sequences\n",
        "    return new_bert\n",
        "\n",
        "def keep_one_word(importance_order, bert_ids, special_ids, **kwargs):\n",
        "\n",
        "    # importance_order gives index, not the actual token_ids\n",
        "\n",
        "    if 'num_words' not in kwargs:\n",
        "        kwargs['num_words'] = 1\n",
        "\n",
        "    new_bert = []\n",
        "    importance_order_ids = []\n",
        "\n",
        "    for idx in importance_order:\n",
        "        if bert_ids.tolist()[idx] not in special_ids:\n",
        "            importance_order_ids.append(bert_ids.tolist()[idx])\n",
        "\n",
        "    to_keep_ids = importance_order_ids[:kwargs['num_words']]\n",
        "\n",
        "\n",
        "    toks = bert_ids.tolist()\n",
        "    new_toks = []\n",
        "\n",
        "    keep_ids = []\n",
        "    keep_ids.extend(special_ids)\n",
        "    keep_ids.extend(to_keep_ids)\n",
        "    for t in toks:\n",
        "        if t not in keep_ids:\n",
        "            continue\n",
        "        else:\n",
        "            new_toks.append(t)\n",
        "    new_bert.append(new_toks)\n",
        "\n",
        "    # Wont return tensor, because we can have different length sequences\n",
        "    return new_bert\n",
        "\n",
        "\n",
        "def repeat_most_important_words(importance_order, bert_ids, special_ids, **kwargs):\n",
        "    # importance_order gives index, not the actual token_ids\n",
        "    if 'topk' not in kwargs:\n",
        "        kwargs['topk'] = 0.5\n",
        "\n",
        "    new_bert = []\n",
        "\n",
        "    importance_order_ids = []\n",
        "\n",
        "    for idx in importance_order:\n",
        "        if bert_ids.tolist()[idx] not in special_ids:\n",
        "            importance_order_ids.append(bert_ids.tolist()[idx])\n",
        "\n",
        "    to_keep = math.ceil(len(importance_order_ids) * kwargs['topk'])\n",
        "    to_keep_ids = importance_order_ids[:to_keep]\n",
        "\n",
        "    toks = bert_ids.tolist()\n",
        "    new_toks = []\n",
        "\n",
        "    keep_ids = []\n",
        "    keep_ids.extend(special_ids)\n",
        "    keep_ids.extend(to_keep_ids)\n",
        "    for t in toks:\n",
        "        if t not in keep_ids:\n",
        "            new_toks.append(random.choice(to_keep_ids))\n",
        "        else:\n",
        "            new_toks.append(t)\n",
        "    new_bert.append(new_toks)\n",
        "\n",
        "    return new_bert\n",
        "\n",
        "def replace_least_important_words(importance_order, bert_ids, special_ids, **kwargs):\n",
        "    # importance_order gives index, not the actual token_ids\n",
        "    if 'topk' not in kwargs:\n",
        "        kwargs['topk'] = 0.5\n",
        "\n",
        "    if 'replace_token_ids' not in kwargs:\n",
        "        raise Exception\n",
        "\n",
        "    replace_token_ids = kwargs['replace_token_ids']\n",
        "\n",
        "    new_bert = []\n",
        "    importance_order_ids = []\n",
        "\n",
        "    for idx in importance_order:\n",
        "        if bert_ids.tolist()[idx] not in special_ids:\n",
        "            importance_order_ids.append(bert_ids.tolist()[idx])\n",
        "\n",
        "    to_keep = math.ceil(len(importance_order_ids) * kwargs['topk'])\n",
        "    to_keep_ids = importance_order_ids[:to_keep]\n",
        "\n",
        "    toks = bert_ids.tolist()\n",
        "    new_toks = []\n",
        "\n",
        "    keep_ids = []\n",
        "    keep_ids.extend(special_ids)\n",
        "    keep_ids.extend(to_keep_ids)\n",
        "    for t in toks:\n",
        "        if t not in keep_ids:\n",
        "            new_toks.append(random.choice(replace_token_ids))\n",
        "        else:\n",
        "            new_toks.append(t)\n",
        "    new_bert.append(new_toks)\n",
        "\n",
        "    return new_bert\n",
        "\n",
        "def most_important_word_in_mention(importance_order, bert_ids1, bert_ids2, special_ids, **kwargs):\n",
        "    # importance_order of premise (sent 1)\n",
        "    new_bert = []\n",
        "    importance_order_ids = []\n",
        "\n",
        "    for idx in importance_order:\n",
        "        if bert_ids1.tolist()[idx] not in special_ids:\n",
        "            importance_order_ids.append(bert_ids1.tolist()[idx])\n",
        "\n",
        "    most_important_tok = importance_order_ids[0]\n",
        "\n",
        "    toks = bert_ids2.tolist()\n",
        "    new_toks = []\n",
        "\n",
        "    keep_ids = []\n",
        "    keep_ids.extend(special_ids)\n",
        "\n",
        "    for t in toks:\n",
        "        if t not in keep_ids:\n",
        "            new_toks.append(most_important_tok)\n",
        "        else:\n",
        "            new_toks.append(t)\n",
        "    new_bert.append(new_toks)\n",
        "\n",
        "    return new_bert\n",
        "\n",
        "def load_examples(filename, input_columns, is_pair=True):\n",
        "    examples = []\n",
        "    header = ''\n",
        "    skipped = 0\n",
        "    with open(filename, 'r') as fp:\n",
        "        for i, line in enumerate(fp):\n",
        "            if i ==0:\n",
        "                header = line.strip()\n",
        "                # Header line\n",
        "                continue\n",
        "            inp = []\n",
        "            line_split = line.strip().split(',')\n",
        "            if max(input_columns) >= len(line_split):\n",
        "                skipped +=1\n",
        "                continue\n",
        "            for col in list(input_columns):\n",
        "                inp.append(line_split[col])\n",
        "            examples.append((line_split, tuple(inp)))\n",
        "\n",
        "    print('Number of examples loaded from file {} = {}'.format(filename, len(examples)))\n",
        "    print('Number of examples skipped from file {} = {}'.format(filename, skipped))\n",
        "\n",
        "    print(header)\n",
        "\n",
        "    return examples, header\n",
        "\n",
        "def get_function_name_dict():\n",
        "    # This is a better way than using eval() or globals()\n",
        "    perturbation_name_func = {\n",
        "        'most_important_word_in_mention' : most_important_word_in_mention, # CopyOne\n",
        "        'keep_most_important_words' : keep_most_important_words, # drop done\n",
        "        'repeat_most_important_words': repeat_most_important_words, # repeat done\n",
        "        'replace_least_important_words' : replace_least_important_words, # replace done\n",
        "        'keep_one_word': keep_one_word,\n",
        "    }\n",
        "\n",
        "    return perturbation_name_func\n",
        "\n",
        "\n",
        "def write_new_examples_to_file(filename, examples, header=None):\n",
        "\n",
        "    fp = open(filename, 'w')\n",
        "    if header is not None:\n",
        "        fp.write(header + '\\n')\n",
        "\n",
        "    for ex in examples:\n",
        "        output_str = ';'.join(ex)\n",
        "        fp.write(output_str + '\\n')\n",
        "\n",
        "    fp.close()\n",
        "\n",
        "from tqdm import tqdm\n",
        "def generate_perturbation_gradient(dataloader, tokenizer, model, filename, input_dir, output_dir, input_columns, perturbation_func, output_filename=None, is_pair=True, on_sent2=True, **kwargs):\n",
        "\n",
        "    examples, header = load_examples(filename, input_columns, is_pair=is_pair)\n",
        "\n",
        "    # See if output_dir exists, otherwise create it\n",
        "    if not os.path.isdir(output_dir):\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    if 'replace_token_ids' not in kwargs:\n",
        "        kwargs['replace_token_ids'] = list(range(tokenizer.vocab_size))[100:20000]\n",
        "\n",
        "    if output_filename is None:\n",
        "        # assume file is tsv\n",
        "        if 'topk' not in kwargs:\n",
        "            kwargs['topk'] = 0.5\n",
        "        output_filename = filename[:-4] + '_' + perturbation_func +  '_' + str(kwargs['topk']) + '.csv'\n",
        "\n",
        "        # If is_pair task, then change filename to include arg on_sent2\n",
        "        if is_pair:\n",
        "            if on_sent2:\n",
        "                if 'topk' not in kwargs:\n",
        "                    kwargs['topk'] = TOP_K\n",
        "                output_filename = filename[:-4] + '_' + perturbation_func + '_sent2' +  '_' + str(kwargs['topk']) + '.csv'\n",
        "            else:\n",
        "                if 'topk' not in kwargs:\n",
        "                    kwargs['topk'] = TOP_K\n",
        "                output_filename = filename[:-4] + '_' + perturbation_func + '_sent1' +  '_' + str(kwargs['topk']) + '.csv'\n",
        "\n",
        "    if os.path.isfile(output_dir + '/' + output_filename):\n",
        "        print('Perturbation File {} already exists !! To override, please delete that file.'.format(output_dir + '/' + output_filename))\n",
        "        return output_filename\n",
        "\n",
        "    func = get_function_name_dict()[perturbation_func]\n",
        "\n",
        "    new_examples = []\n",
        "    overall_count = 0\n",
        "\n",
        "    if not is_pair and perturbation_func == 'most_important_word_in_mention':\n",
        "        print('Perturbation : {} Invalid for single sentence. Return None '.format(perturbation_func))\n",
        "        return None\n",
        "\n",
        "    for i, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(dataloader)):\n",
        "        if i %100 == 0:\n",
        "            print('Done with {0}/{1} batches'.format(i, len(dataloader)))\n",
        "       \n",
        "\n",
        "        seq, attn_masks, token_type_ids, labels = \\\n",
        "                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n",
        "\n",
        "        grads = get_gradients(model, seq, attn_masks, token_type_ids, labels)\n",
        "\n",
        "        if not is_pair:\n",
        "            # If a single sentence task like SST-2\n",
        "            importance_order_bert, bert_tok, bert_grads = get_importance_order(model, grads, seq, attn_masks, token_type_ids, labels, is_pair=is_pair)\n",
        "            for j in range(len(importance_order_bert)):\n",
        "                ex = examples[overall_count]\n",
        "                new_bert_ids = func(importance_order_bert[j], bert_tok[j],\n",
        "                                                        tokenizer.all_special_ids, **kwargs)[0]\n",
        "                new_sent = tokenizer.decode(new_bert_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "                new_cols = ex[0]\n",
        "                new_cols[input_columns[0]] = new_sent\n",
        "                new_examples.append(new_cols)\n",
        "                overall_count +=1\n",
        "\n",
        "        else:\n",
        "            #print(len(examples))\n",
        "            importance_order_bert1, importance_order_bert2, bert1_tok, bert2_tok, _, _ = get_importance_order(model, grads, seq, attn_masks, token_type_ids, labels, is_pair=is_pair)\n",
        "            for j in range(len(importance_order_bert2)):\n",
        "                #print(overall_count)\n",
        "                ex = examples[overall_count]\n",
        "                new_cols = ex[0]\n",
        "                if on_sent2:\n",
        "                    # For on_sent2=True, copy sent1 in sent2 and then apply another perturbation (like sort)\n",
        "                    if perturbation_func == 'most_important_word_in_mention':\n",
        "                        new_bert_ids = func(importance_order_bert1[j], bert1_tok[j], bert2_tok[j],\n",
        "                                            tokenizer.all_special_ids, **kwargs)[0]\n",
        "                    else:\n",
        "\n",
        "                        new_bert_ids = func(importance_order_bert2[j], bert2_tok[j],\n",
        "                                                        tokenizer.all_special_ids, **kwargs)[0]\n",
        "                    new_sent = tokenizer.decode(new_bert_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "                    new_cols[input_columns[1]] = new_sent\n",
        "                else:\n",
        "                    # For on_sent2=False, copy sent2 in sent1 and then apply another perturbation (like sort)\n",
        "                    if perturbation_func == 'most_important_word_in_mention':\n",
        "                        new_bert_ids = func(importance_order_bert2[j], bert2_tok[j], bert1_tok[j],\n",
        "                                            tokenizer.all_special_ids, **kwargs)[0]\n",
        "                    else:\n",
        "                        new_bert_ids = func(importance_order_bert1[j], bert1_tok[j],\n",
        "                                                        tokenizer.all_special_ids, **kwargs)[0]\n",
        "                    new_sent = tokenizer.decode(new_bert_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "                    new_cols[input_columns[0]] = new_sent\n",
        "                new_examples.append(new_cols)\n",
        "                overall_count +=1\n",
        "\n",
        "\n",
        "    write_new_examples_to_file(output_dir + '/' + output_filename, new_examples, header=None)\n",
        "    print(len(new_examples))\n",
        "    print('Successfully generated perturbations for funcion : {} with params (is_pair={}, on_sent2={}) saved to file {}'.format(\n",
        "                                                                        perturbation_func, str(is_pair), str(on_sent2), output_filename))\n",
        "\n",
        "    return output_filename"
      ],
      "metadata": {
        "id": "JhmhguNOZ89c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}